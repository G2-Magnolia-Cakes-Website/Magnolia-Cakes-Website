#!/usr/bin/python
"""
This program moves local files to and from Amazon's Simple Storage
System.

Capability:
  - Test if file exists in S3 bucket
  - Upload file to S3 bucket
  - Download file from S3 bucket
  - Delete file from S3 bucket
  - Copy S3 file to S3 file
  - Update metadata of S3 file

  - Create S3 bucket
  - Delete S3 bucket
  - Configure S3 bucket
  - List S3 bucket keys

  - List S3 buckets

S3 files also have metadata in addition to their content.
metadata is a python dict i.e. a set of key, value pairs.

IMPLEMENTATION NOTES:
The implementation is based on tinys3.  I've kept the connection
and request objects but changed their API so it is not compatible
with tinys3 at all.  I do use tinys3's util module unchanged, and
auth module with a slight change to the signing logic.

Paul Wexler, Jun 3, 2014
"""
from .auth import S3Auth
from .d2x import DictToXML
from .util import LenWrapperStream

import base64
import hashlib
import httplib
import json
import logging
import math
import os
import requests
import time
import xmltodict

#httplib.HTTPConnection.debuglevel = 1

MB = 1024 * 1024
GB = 1024 * MB
TB = 1024 * GB

class Retry(object):
    """Manage retries.
    Retry only if the response status code is in the list.
    Retry until the count is 0.

    limit
        The number of attempts to try.  0 means unlimited attempts.

    interval
        The number of seconds (floating point) to wait between attempts.

    status_codes
        A list of status codes to retry. 
    """
    def __init__(self, limit, interval, status_codes):
        self.limit = limit
        self.interval = interval
        self.status_codes = [int(c) for c in status_codes]
        self.reset()
        
    def next(self, status_code, method, url, message):
        """
        manages count.  
        conditionally logs retry message and sleeps for interval.
        """
        if int(status_code) in self.status_codes:
            if self.limit:
                self.count -= 1
            if self.count and self.interval:
                logging.debug('retrying %s %s: %s' % (method, url, message))
                time.sleep(self.interval)
        else:
            self.count = 0

    def reset(self):
        """
        resets count.
        """
        self.count = self.limit if self.limit else 1
        
DEFAULT_RETRY = Retry(5, 2.5, [104,])
                
class S3Bucket:
    """The bucket object derived from the S3 <Bucket> tag dict.

    attributes:

    - name
    - creation_date
    """
    def __init__(self, bucket={}):
        self.name = bucket.get('Name', '')
        self.creation_date = bucket.get('CreationDate', '')

class S3Facts:
    """
    Facts (i.e. constants) for S3
    """
    amz_metadata_prefix = 'x-amz-meta-'
    default_endpoint = 's3.amazonaws.com'
    default_region = 'us-east-1'
    maximum_file_size = 5 * TB
    maximum_part_number = 10 * 1000
    maximum_part_size = 5 * GB
    minimum_part_size = 5 * MB
    multipart_threshhold = 100 * MB

class S3Key:
    """The key object listed in the <Contents> tag returned by the GET
    BUCKET request.

    attributes:

    - key
    - last_modified
    - e_tag
    - size
    - owner - an S3Owner instance
    - storage_class
    """
    def __init__(self, obj={}):
        self.key = obj.get('Key', '')
        self.last_modified = obj.get('LastModified', '')
        self.e_tag = obj.get('ETag', '')
        self.size = obj.get('Size', '')
        self.owner = S3Owner(obj.get('Owner', {}))
        self.storage_class = obj.get('StorageClass', '')

class S3Name:
    """An S3 file name consists of a bucket and a key.  This pair
    uniquely identifies the file within S3.

    The S3Connection class methods take a remote_name argument
    which can be either a string which is the key, or an instance
    of the S3Name class.  When no bucket is given the
    default_bucket established when the connection is instantiated
    is used.

    In other words, the S3Name class provides a means of using a
    bucket other than the default_bucket.
    """
    def __init__(self, key, bucket=None):
        self.key = key
        self.bucket = bucket

    def __str__(self):
        return '%s/%s' % (self.bucket, self.key)

class S3Owner:
    """The Owner object in the key object listed in the <Contents> tag
    returned by the GET BUCKET request.

    attributes:

    - id
    - display_name
    """
    def __init__(self, obj={}):
        self.id = obj.get('ID', '')
        self.display_name = obj.get('DisplayName', '')

class S3Part:
    """
    part_number
        ranges from 1 to S3Facts.maximum_part_number

    etag
        returned by S3 for successful upload_part
    """
    def __init__(self, part_number, etag):
        self.part_number = part_number
        self.etag = etag

    def __str__(self):
        return 'part_number: %s, etag: %s' % (self.part_number, self.etag)

class S3Request(object):
    """Base request object.
    Altered directly from the tinys3.S3Request implementation.
    """
    def __init__(self, conn):
        self.auth = conn.auth
        self.tls = conn.tls
        self.endpoint = conn.endpoint
        self.region = conn.region
        self.retry = conn.retry

    def _bucket_url(self, bucket, key):
        protocol = 'https' if self.tls else 'http'
        return "%s://%s/%s/%s" % (
                protocol,
                self.endpoint,
                bucket,
                key.lstrip('/'))

    def _get_metadata(self, headers):
        return dict((k, headers[k]) for k in headers
                if k.startswith(S3Facts.amz_metadata_prefix) )

    def _send_request(
            self,
            method,
            url,
            params=None,
            data=None,
            headers=None,
            cookies=None,
            files=None,
            auth=None,
            timeout=None,
            allow_redirects=None,
            proxies=None,
            verify=None,
            stream=None,
            cert=None):
        logging.debug(['_send_request', method, url, headers, params])
        self.retry.reset()
        while self.retry.count:
            try:
                r = requests.request(
                        method=method,
                        url=url,
                        params=params,
                        data=data,
                        headers=headers,
                        cookies=cookies,
                        files=files,
                        auth=auth,
                        timeout=timeout,
                        allow_redirects=allow_redirects,
                        proxies=proxies,
                        verify=verify,
                        stream=stream,
                        cert=cert,
                        )
                self.retry.next(
                        r.status_code, 
                        method, 
                        url, 
                        '%s: %s' % (r.reason, r.text))
            except Exception, e:
                self.retry.next(
                        e[0],
                        method,
                        url,
                        str(e))
                if not self.retry.count:
                    raise
        return r

    def send(
            self,
            method,
            url,
            params=None,
            data=None,
            headers=None,
            **kwargs):
        r = self._send_request(
                method,
                url,
                auth=self.auth,
                params=params,
                headers=headers,
                data=data,
                **kwargs)
        ok = r.status_code in [200, 204]
        error = r.reason if ok else 'unable to %s %s: %i %s: %s' % (
                method,
                url,
                r.status_code,
                r.reason,
                r.text)
        return S3Response(ok=ok, error=error, response=r)

class S3Response:
    """
    ok
        boolean - the response was/not ok.
    error
        the error when the response was not ok.
    response
        the response object returned by the requests module.
    """
    def __init__(self, ok=True, error='', response=None, **kwargs):
        self.ok = ok
        self.error = error
        self.response = response
        self.__dict__.update(kwargs)

def get_content_md5(s):
    """
    Returns the base64-encoded 128-bit MD5 digest of the string s.
    """
    m = hashlib.md5()
    m.update(s)
    return base64.b64encode(m.digest())

def get_content_md5_header(s):
    return {'Content-MD5': get_content_md5(s)}

class BucketS3Request(S3Request):
    """
    send() returns S3Response instance
    """
    def __init__(self, conn):
        super(BucketS3Request, self).__init__(conn)

    def _get_bucket_url(self, bucket):
        protocol = 'https' if self.tls else 'http'
        return "%s://%s.%s%s.%s" % (
                protocol,
                bucket,
                's3-',
                self.region,
                'amazonaws.com')

    def send(
            self,
            method,     # 'DELETE', 'PUT', etc.
            bucket,     # name of the bucket
            params={},  #  {} the ? & parameters, or a string (not including ?)
            headers={}, #  {} additional request headers
            data=None,  #  a string - the body of the message
            **kwargs):
        url = self._get_bucket_url(bucket)
        r = self._send_request(
                method,
                url,
                auth=self.auth,
                params=params,
                headers=headers,
                data=data,
                **kwargs)
        ok = r.status_code in [200, 204]
        error = r.reason if ok else 'unable to %s BUCKET %s: %i %s: %s' % (
                method,
                url,
                r.status_code,
                r.reason,
                r.text)
        return S3Response(ok=ok, error=error, response=r)

class CopyS3Request(S3Request):
    """
    send() returns S3Response instance
    """
    def __init__(self,
            conn,
            src_bucket, src_key,
            dst_bucket, dst_key,
            headers={},
            ):

        super(CopyS3Request, self).__init__(conn)
        self.src_bucket = src_bucket
        self.src_key = src_key
        self.dst_bucket = dst_bucket
        self.dst_key = dst_key
        self.headers = headers

        self.metadata = self._get_metadata(headers)
        self.url = self._bucket_url(self.dst_bucket, self.dst_key)

    def send(self):
        remote_src = '/%s/%s' % (self.src_bucket, self.src_key)
        metadata_directive = 'REPLACE' if self.metadata else 'COPY'
        headers = {
                'x-amz-copy-source': remote_src,
                'x-amz-metadata-directive': metadata_directive,
                }

        if self.headers:
            headers.update(self.headers)

        logging.debug(['COPY', self.url, headers])

        r = self._send_request(
                'PUT',
                self.url,
                auth=self.auth,
                headers=headers)

        ok = False
        if r.status_code == 200:
            result = xmltodict.parse(r.content)
            if 'CopyObjectResult' in result:
                if 'ETag' in result ['CopyObjectResult']:
                    ok = True
        else:
            result = r.reason

        error = '' if ok else 'unable to COPY %s to %s: %i %s: %s' % (
                remote_src,
                self.url,
                r.status_code,
                result,
                r.text)
        return S3Response(ok=ok, error=error, response=r)

class DeleteS3Request(S3Request):
    """
    send() returns S3Response instance
    """
    def __init__(self,  conn, bucket, key):
        super(DeleteS3Request, self).__init__(conn)
        self.bucket = bucket
        self.key = key
        self.url = self._bucket_url(self.bucket, self.key)

    def send(self):
        logging.debug(['DELETE', self.url])
        r = self._send_request(
                'DELETE',
                self.url,
                auth=self.auth)
        ok = r.status_code == 204
        error = r.reason if ok else 'unable to DELETE %s: %i %s: %s' % (
                self.url,
                r.status_code,
                r.reason,
                r.text)
        return S3Response(ok=ok, error=error, response=r)

class ExistsS3Request(S3Request):
    """
    send() returns S3Response instance
    """
    def __init__(self,  conn, bucket, key):
        super(ExistsS3Request, self).__init__(conn)
        self.bucket = bucket
        self.key = key
        self.url = self._bucket_url(self.bucket, self.key)

    def send(self):
        logging.debug(['HEAD', self.url])
        r = self._send_request(
                'HEAD',
                self.url,
                auth=self.auth)
        if r.status_code == 200:
            ok = True
            error = ''
        elif r.status_code == 404:
            ok = True
            error = r.reason
        else:
            ok = False
            error = 'unable to HEAD %s: %i %s: %s' % (
                    self.url,
                    r.status_code,
                    r.reason,
                    r.text)
        metadata = self._get_metadata(r.headers)
        return S3Response(ok=ok, error=error, response=r, metadata=metadata)

class ReadS3Request(S3Request):
    """
    send() returns S3Response instance which includes a metadata
    attribute containing the metadata of the bucket/key.
    """
    def __init__(self,  conn, bucket, key, local_file):
        super(ReadS3Request, self).__init__(conn)
        self.bucket = bucket
        self.key = key
        self.fp = local_file
        self.url = self._bucket_url(self.bucket, self.key)
        self.part_size = S3Facts.multipart_threshhold

    def send(self):
        logging.debug(['GET', self.url])
        r = self._send_request(
                'GET',
                self.url,
                auth=self.auth,
                stream=True)
        ok = r.status_code == 200
        if ok:
            for x in r.iter_content(chunk_size=self.part_size):
                self.fp.write(x)
        error = r.reason if ok else 'unable to GET %s: %i %s: %s' % (
                self.url,
                r.status_code,
                r.reason,
                r.text)
        metadata = self._get_metadata(r.headers)
        return S3Response(ok=ok, error=error, response=r, metadata=metadata)

class UpdateMetadataS3Request(CopyS3Request):
    """
    send() returns S3Response instance
    """
    def __init__(self,  conn, bucket, key, headers={}):
        super(UpdateMetadataS3Request, self).__init__(
                conn,
                bucket, key,
                bucket, key,
                headers=headers,
                )

class WriteS3Request(S3Request):
    """
    send() returns S3Response instance
    """
    def __init__(self,  conn, local_file, bucket, key, headers={}):
        super(WriteS3Request, self).__init__(conn)

        self.conn = conn
        self.fp = local_file
        self.bucket = bucket
        self.key = key
        self.headers = headers

        self.data = LenWrapperStream(self.fp)
        self.url = self._bucket_url(self.bucket, self.key)

    def send(self):
        nbytes = len(self.data)
        if nbytes < S3Facts.multipart_threshhold:
            return self._upload()
        else:
            assert nbytes <= S3Facts.maximum_file_size
            # compute the smallest part_size that will accomodate the file.
            part_size = max(
                    S3Facts.minimum_part_size,
                    int(math.ceil(float(nbytes) / S3Facts.maximum_part_number))
                    )
            self._begin_multipart()
            try:
                while nbytes:
                    part_size = min(part_size, nbytes)
                    part = self.fp.read(part_size)
                    self._upload_part(part_size, part)
                    nbytes -= part_size
                return self._end_multipart()
            except Exception, e:
                upload = '%s %s/%s' % (self.upload_id, self.bucket, self.key)
                s3_response = S3Response()
                try:
                    s3_response = self._abort_multipart()
                    if s3_response.ok:
                        s3_response.ok = False
                        error = "Upload %s aborted after exception %s" % (
                                upload,
                                e)
                        s3_response.error = error
                        return s3_response
                    else:
                        pass
                except Exception, e2:
                    s3_response.ok = False
                    s3_response.error = e2

                error = ('%s - Unable to abort %s: %s, after exception %s') % (
                        "CRITICAL ERROR",
                        upload,
                        s3_response.error,
                        e)
                s3_response.error = error
                return s3_response

    def _abort_multipart(self):
        logging.debug(['_abort_multipart', self.url])
        params = { 'uploadId': self.upload_id, }
        r = self._send_request(
                'DELETE',
                self.url,
                params=params,
                auth=self.auth)
        s3_response = S3Response(response=r)
        if r.status_code == 204:
            s3_response.ok = True
        else:
            s3_response.ok = False
            s3_response.error = r.reason
        return s3_response

    def _begin_multipart(self):
        logging.debug(['_begin_multipart', self.url])
        params = 'uploads'
        r = self._send_request(
                'POST',
                self.url,
                headers=self.headers,
                params=params,
                auth=self.auth)
        r.raise_for_status()
        result = xmltodict.parse(r.content)
        self.upload_id = result['InitiateMultipartUploadResult']['UploadId']
        self.part_number = 0
        self.parts = []
        logging.debug(['upload_id', self.upload_id])

    def _end_multipart(self):
        logging.debug(['_end_multipart', self.url])
        data = '<CompleteMultipartUpload>\n'
        for part in self.parts:
            data += '  <Part>\n'
            data += '    <PartNumber>%s</PartNumber>\n' % part.part_number
            data += '    <ETag>%s</ETag>\n' % part.etag
            data += '  </Part>\n'
        data += "</CompleteMultipartUpload>"
        params = { 'uploadId': self.upload_id, }
        headers = { 'Content-Length': len(data) }
        r = self._send_request(
                'POST',
                self.url,
                headers=headers,
                params=params,
                data=data,
                auth=self.auth)
        logging.info('\n' + data)
        if r.status_code != 200:
            raise Exception, "Unable to CompleteMultipartUpload: %s %s: %s" % (
                r.status_code,
                r.reason,
                r.text)
        logging.info('Upload of %s %s.' % (self.url, r.reason))
        return S3Response(
                r.status_code == 200,
                r.reason,
                r)

    def _upload(self):
        logging.debug(['_upload', self.url])
        r = self._send_request(
                'PUT',
                self.url,
                data=self.data,
                auth=self.auth,
                headers=self.headers)
        if r.status_code == 200:
            ok = True
            error = ''
        else:
            ok = False
            error = 'unable to PUT %s: %i %s: %s' % (
                    self.url,
                    r.status_code,
                    r.reason,
                    r.text)
        return S3Response(
                ok=ok,
                error=error,
                response=r)

    def _upload_part(self, part_size, part):
        """ headers:
        Content-Length
        Content-MD5
        """
        logging.debug(['_upload_part', part_size, part[:10]])
        headers = {}
        params = {}

        headers['Content-Length'] = part_size
        headers['Content-MD5'] = get_content_md5(part)

        self.part_number += 1
        assert self.part_number <= S3Facts.maximum_part_number

        params['partNumber'] = self.part_number
        params['uploadId'] = self.upload_id
        try:
            r = self._send_request(
                    'PUT',
                    self.url,
                    headers=headers,
                    params=params,
                    data=part,
                    auth=self.auth)
            logging.debug([
                    '_upload_part',
                    r.status_code,
                    r.reason,
                    params['partNumber']])
        except Exception, e:
            raise Exception, "Unable to PUT.  Exception: %s" % e
        if r.status_code != 200:
            raise Exception, "Unable to PUT (upload part): %s %s: %s" % (
                    r.status_code,
                    r.reason,
                    r.text)

        s3_part = S3Part(self.part_number, r.headers['Etag'])
        self.parts.append(s3_part)
        logging.info('Uploaded part %s' % s3_part)


class S3Connection(object):
    """
    Altered directly from the tinys3.connection implementation.
        
    Creates a new S3 connection

    Params:

    access_key_id
        AWS access key (username)
    secret_access_key
        AWS secret access key (password)
    default_bucket
        Sets the default bucket.

        Default is None
    endpoint
        Sets the s3 endpoint.

        Default is S3Facts.default_endpoint.
    tls
        do/not use secure connection.

        Default is True.
    region
        Region associated with the endpoint.

        Default is S3Facts.default_region.
    retry
        dict contains values to retry requests.request()

        Default is {'limit': 5, 'interval': 2.5, 'status_codes': [104,]}

        limit
            Number of times to retry. 0 means unlimited.
        interval
            Seconds to wait between retries.
        status_codes
            List of status codes (errors) to retry on.
    """

    def __init__(
            self,
            access_key_id,
            secret_access_key,
            default_bucket=None,
            endpoint=S3Facts.default_endpoint,
            tls=True,
            region=S3Facts.default_region,
            retry={},
            ):
        self.auth = S3Auth(access_key_id, secret_access_key)
        self.default_bucket = default_bucket
        self.endpoint = endpoint
        self.tls = tls
        self.region = region
        self.retry = Retry(
                retry.get('limit', DEFAULT_RETRY.limit),
                retry.get('interval', DEFAULT_RETRY.interval),
                retry.get('status_codes', DEFAULT_RETRY.status_codes),)
                
    def bucket_request(
            self,
            method,
            bucket,
            headers={},
            params={},
            data=None,
            **kwargs):
        r = BucketS3Request(self)
        s3_response = r.send(
                method,
                bucket,
                headers=headers,
                params=params,
                data=data,
                **kwargs)
        return s3_response

    def copy(self, remote_src, remote_dst, headers={}):
        src_bucket, src_key = self._get_bucket_and_key(remote_src)
        dst_bucket, dst_key = self._get_bucket_and_key(remote_dst)
        r = CopyS3Request(
                self,
                src_bucket, src_key,
                dst_bucket, dst_key,
                headers=headers)
        s3_response = r.send()
        return s3_response

    def delete(self, remote_name):
        bucket, key = self._get_bucket_and_key(remote_name)
        r = DeleteS3Request(self, bucket, key)
        s3_response = r.send()
        return s3_response

    def exists(self, remote_name):
        bucket, key = self._get_bucket_and_key(remote_name)
        r = ExistsS3Request(self, bucket, key)
        s3_response = r.send()
        return s3_response

    def read(self, remote_name, local_file):
        bucket, key = self._get_bucket_and_key(remote_name)
        r = ReadS3Request(self, bucket, key, local_file)
        s3_response = r.send()
        return s3_response

    def request(
            self,
            method,
            url,
            headers={},
            params={},
            data=None,
            **kwargs):
        r = S3Request(self)
        s3_response = r.send(
                method,
                url,
                headers=headers,
                params=params,
                data=data,
                **kwargs)
        return s3_response

    def update_metadata(self, remote_name, headers):
        bucket, key = self._get_bucket_and_key(remote_name)
        r = UpdateMetadataS3Request(self, bucket, key, headers)
        s3_response = r.send()
        return s3_response

    def write(self, local_file, remote_name, headers={}):
        bucket, key = self._get_bucket_and_key(remote_name)
        r = WriteS3Request(
                self,
                local_file,
                bucket,
                key,
                headers=headers)
        s3_response = r.send()
        return s3_response

    def _get_bucket(self, bucket):
        """
        Verifies that we have a bucket for a request

        Params:

        bucket
            The name of the bucket we're trying to use,
            None if we want to use the default bucket.
        Returns
            The bucket to use for the request
        Raises
            ValueError if no bucket was provided AND no
            default bucket was defined.
        """
        b = bucket or self.default_bucket
        if not b:
            raise ValueError(
                    "You must specify a bucket in your request "
                    "or set the default_bucket for the connection")
        return b

    def _get_bucket_and_key(self, remote_name):
        if isinstance(remote_name, str):
            bucket = None
            key = remote_name
        elif isinstance(remote_name, S3Name):
            bucket = remote_name.bucket
            key = remote_name.key
        else:
            raise Exception, 'invalid remote_name: %s' % remote_name
        return self._get_bucket(bucket), key

class StorageError(Exception):
    def __init__(self, msg, exception, response=None):
        self.msg = msg
        self.exception = exception
        self.response = response

    def __str__(self):
        return '%s: %s' % (self.msg, self.exception)

def check_response(f):
    """
    The decorated function adds basic response handling to a function
    returning an S3Response object.
    It also saves the requests response object in self.response
    """
    def decorated_f(self, *args, **kwargs):
        try:
            response = f(self, *args, **kwargs)
            self.response = response.response
            if response.ok:
                return
            else:
                raise StorageError(
                        f.__name__, 
                        response.error, 
                        response=response.response)
        except Exception, e:
            raise StorageError(f.__name__, e)
    return decorated_f

def check_response_ok(f):
    """
    The decorated function adds basic response handling to a function
    returning an S3Response object.
    It also saves the requests response object in self.response
    It also returns self.response.ok
    """
    def decorated_f(self, *args, **kwargs):
        try:
            response = f(self, *args, **kwargs)
            self.response = response.response
            return response.ok
        except Exception, e:
            raise StorageError(f.__name__, e)
    return decorated_f

def get_json_content(f):
    """
    The decorated function returns a dict converted from the json string in
    the response content.
    """
    def decorated_f(self, *args, **kwargs):
        f(self, *args, **kwargs)
        return json.loads(self.response.content)
    return decorated_f

def get_xml_content(f):
    """
    The decorated function returns a dict converted from the xml string in
    the response content.
    """
    def decorated_f(self, *args, **kwargs):
        f(self, *args, **kwargs)
        return xmltodict.parse(self.response.content)
    return decorated_f

class Storage(object):
    """
    """
    def __init__(self, connection, **kwargs):
        self.connection = connection
        self.response = None
        self.parser = DictToXML()

    def _to_json(self, obj):
        return (obj
                if type(obj) in [str, type(None)]
                else json.dumps(obj))

    def _to_xml(self, obj):
        return (obj
                if type(obj) in [str, type(None)]
                else self.parser.to_xml(obj))

    @check_response
    def bucket_create(self, bucket, headers={}, data=None):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html
        """
        my_data = data or {
                'CreateBucketConfiguration'
                ' xmlns="http://s3.amazonaws.com/doc/2006-03-01/"': {
                        'LocationConstraint': self.connection.region}}

        return self.connection.bucket_request(
                'PUT',
                bucket,
                headers=headers,
                data=self._to_xml(my_data))

    @check_response
    def bucket_delete(self, bucket):
        """
       http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETE.html

       Delete bucket
        """
        return self.connection.bucket_request('DELETE', bucket)

    @check_response
    def bucket_delete_cors(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEcors.html
        Deletes the cors configuration information set for the bucket
        """
        return self.connection.bucket_request(
                'DELETE',
                bucket,
                params='cors')

    @check_response
    def bucket_delete_lifecycle(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETElifecycle.html
        Deletes the lifecycle configuration information set for the bucket
        """
        return self.connection.bucket_request(
                'DELETE',
                bucket,
                params='lifecycle')

    @check_response
    def bucket_delete_policy(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEpolicy.html
        Deletes the policy for the bucket
        """
        return self.connection.bucket_request(
                'DELETE',
                bucket,
                params='policy')

    @check_response
    def bucket_delete_tagging(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEtagging.html
        Deletes the tags set for the bucket
        """
        return self.connection.bucket_request(
                'DELETE',
                bucket,
                params='tagging')

    @check_response
    def bucket_delete_website(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEwebsite.html
        Deletes the website configuration information set for the bucket
        """
        return self.connection.bucket_request(
                'DELETE',
                bucket,
                params='website')

    @check_response_ok
    def bucket_exists(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketHEAD.html
        Returns True/false as the bucket does/not exist.
        """
        return self.connection.bucket_request('HEAD', bucket)

    @check_response
    def bucket_get(self, bucket, params={}, **kwargs):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html
        Gets the next block of keys from the bucket based on params.
        """
        return self.connection.bucket_request(
                'GET',
                bucket,
                params=params,
                **kwargs)

    @get_xml_content
    @check_response
    def bucket_get_acl(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETacl.html
        Returns bucket acl as a dict.
        self.response.content is the result in XML.
        """
        return self.connection.bucket_request(
                'GET',
                bucket,
                params="acl")

    @get_xml_content
    @check_response
    def bucket_get_cors(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETcors.html
        Returns bucket cors as a dict.
        self.response.content is the result in XML.
        """
        return self.connection.bucket_request(
                'GET',
                bucket,
                params="cors")

    @get_xml_content
    @check_response
    def bucket_get_lifecycle(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlifecycle.html
        Returns bucket lifecycle as a dict.
        self.response.content is the result in XML.
        """
        return self.connection.bucket_request(
                'GET',
                bucket,
                params="lifecycle")

    @get_xml_content
    @check_response
    def bucket_get_location(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlocation.html
        Returns bucket location as a dict.
        self.response.content is the result in XML.
        """
        return self.connection.bucket_request(
                'GET',
                bucket,
                params="location")

    @get_xml_content
    @check_response
    def bucket_get_logging(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlogging.html

        Returns bucket logging as a dict.
        self.response.content is the result in XML.
        """
        return self.connection.bucket_request(
                'GET',
                bucket,
                params="logging")

    @get_xml_content
    @check_response
    def bucket_get_notification(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETnotification.html
        Returns bucket notification as a dict.
        self.response.content is the result in XML.
        """
        return self.connection.bucket_request(
                'GET',
                bucket,
                params="notification")

    @get_json_content
    @check_response
    def bucket_get_policy(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETpolicy.html
        Returns bucket policy as a dict.
        self.response.content is the result in json.
        """
        return self.connection.bucket_request(
                'GET',
                bucket,
                params="policy")

    @get_xml_content
    @check_response
    def bucket_get_request_payment(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTrequestPaymentGET.html
        Returns bucket requestPayment configuration as a dict.
        self.response.content is the result in XML.
        """
        return self.connection.bucket_request(
                'GET',
                bucket,
                params="requestPayment")

    @get_xml_content
    @check_response
    def bucket_get_tagging(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETtagging.html
        Returns bucket tagging as a dict.
        self.response.content is the result in XML.
        """
        return self.connection.bucket_request(
                'GET',
                bucket,
                params="tagging")

    @get_xml_content
    @check_response
    def bucket_get_versioning(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETversioningStatus.html
        Returns bucket versioning status as a dict.
        self.response.content is the result in XML.
        """
        return self.connection.bucket_request(
                'GET',
                bucket,
                params="versioning")

    @get_xml_content
    @check_response
    def bucket_get_versions(self, bucket, params={}):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETVersion.html
        Returns metadata about the versions of objects in a bucket as a dict.
        self.response.content is the result in XML.
        """
        return self.connection.bucket_request(
                'GET',
                bucket,
                params='&'.join(
                        ['versions'] + ['%s=%s' % x for x in params.items()]))

    @get_xml_content
    @check_response
    def bucket_get_website(self, bucket):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETwebsite.html
        Returns bucket website configuration as a dict.
        self.response.content is the result in XML.
        """
        return self.connection.bucket_request(
                'GET',
                bucket,
                params="website")

    @get_xml_content
    @check_response
    def _bucket_list(self):
        """
        Lists all the buckets for this account.
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTServiceGET.html
        """
        return self.connection.request(
                'GET',
                'http://' + S3Facts.default_endpoint)

    def bucket_list(self):
        """
        Returns a generator which returns an S3Bucket instance for each
        bucket in the account of the authenticated user.

        You gotta luv xml.
        If there are no buckets 'Buckets' is an empty tag, otherwise it
        contains a 'Bucket' tag.
        If there is 1 bucket, 'Bucket' contains a dict.
        If there is more than 1 bucket, 'Bucket' is a list of dicts.
        """
        x = self._bucket_list()
        buckets = x['ListAllMyBucketsResult']['Buckets'] or []
        if buckets:
            buckets = buckets['Bucket']
            if not isinstance(buckets, list):
                buckets = [buckets]
        for bucket in buckets:

            yield S3Bucket(bucket)

    def bucket_list_keys(
            self,
            bucket,
            delimiter=None,
            prefix=None,
            params={},
            **kwargs):
        """
        Returns a generator which returns the keys in the given bucket.

        If delimiter is used, the keys are returned first, followed by
        the common prefixes.  Each key is returned as an instance of S3Key
        while each common prefix is returned as a string.
        """
        self._blocks_remain = True
        self._marker = params.get('marker', '')
        my_params = params.copy()
        my_params['delimiter'] = delimiter
        my_params['prefix'] = prefix
        while self._blocks_remain:
            my_params['marker'] = self._marker
            block = self._get_next_bucket_block(
                    bucket,
                    params=my_params,
                    **kwargs)
            for key in block:
                yield S3Key(key) if 'Key' in key else key['Prefix']

    def _get_next_bucket_block(
                self,
                bucket,
                params,
                **kwargs):
        """
        Makes the self.bucket_get() call.
        Uses xmltodict on the response content to extract the list of items
        to return.
        Sets self._blocks_remain to False when the last block is fetched.
        Manages self._marker to know what to ask for next time.
        Appends CommonPrefixes if they are returned by S3.
        Based on deciphering:
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html
        """
        self.bucket_get(bucket, params=params, **kwargs)
        content = xmltodict.parse(self.response.content)
        result = content['ListBucketResult']
        self._blocks_remain = (result['IsTruncated'] == 'true')
        contents = result.get('Contents', [])
        if not isinstance(contents, list):
            contents = [contents]
        if contents:
            self._marker = contents[-1]['Key']
        common_prefixes = result.get('CommonPrefixes', [])
        if not isinstance(common_prefixes, list):
            common_prefixes = [common_prefixes]
        if common_prefixes:
            contents.extend(common_prefixes)
        return contents

    @check_response
    def bucket_set_acl(self, bucket, headers={}, data=None):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTacl.html

        Configure bucket acl using xml data, or request headers.
        Supply either headers or data.

        data may be a string or a dict.
        """
        return self.connection.bucket_request(
                'PUT',
                bucket,
                params="acl",
                headers=headers,
                data=self._to_xml(data))

    @check_response
    def bucket_set_cors(self, bucket, data=None):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTcors.html

        Configure bucket cors with xml data.
        data may be a string or a dict.
        """
        return self.connection.bucket_request(
                'PUT',
                bucket,
                params="cors",
                headers=get_content_md5_header(data),
                data=self._to_xml(data))

    @check_response
    def bucket_set_lifecycle(self, bucket, data=None):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlifecycle.html

        Configure bucket lifecycle with xml data.
        data may be a string or a dict.
        """
        return self.connection.bucket_request(
                'PUT',
                bucket,
                params="lifecycle",
                headers=get_content_md5_header(data),
                data=self._to_xml(data))

    @check_response
    def bucket_set_logging(self, bucket, data=None):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTpolicy.html

        Configure bucket logging with xml data.
        data may be a string or a dict.
        """
        return self.connection.bucket_request(
                'PUT',
                bucket,
                params="logging",
                data=self._to_xml(data))

    @check_response
    def bucket_set_notification(self, bucket, data=None):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTnotification.html

        Configure bucket notification with xml data.
        data may be a string or a dict.
        """
        return self.connection.bucket_request(
                'PUT',
                bucket,
                params="notification",
                data=self._to_xml(data))

    @check_response
    def bucket_set_policy(self, bucket, data=None):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTpolicy.html

        Configure bucket policy using json data.
        data may be a string or a dict.
        """
        return self.connection.bucket_request(
                'PUT',
                bucket,
                params="policy",
                data=self._to_json(data))

    @check_response
    def bucket_set_request_payment(self, bucket, data=None):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTrequestPaymentPUT.html

        Configure bucket requestPayment with xml data.
        data may be a string or a dict.
        """
        return self.connection.bucket_request(
                'PUT',
                bucket,
                params="requestPayment",
                data=self._to_xml(data))

    @check_response
    def bucket_set_tagging(self, bucket, data=None):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTtagging.html

        Configure bucket tagging with xml data.
        data may be a string or a dict.
        """
        return self.connection.bucket_request(
                'PUT',
                bucket,
                params="tagging",
                data=self._to_xml(data))

    @check_response
    def bucket_set_versioning(self, bucket, headers={}, data=None):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTVersioningStatus.html

        Configure bucket versioning using xml data and request headers.

        data may be a string or a dict.
        """
        return self.connection.bucket_request(
                'PUT',
                bucket,
                params="versioning",
                headers=headers,
                data=self._to_xml(data))

    @check_response
    def bucket_set_website(self, bucket, data=None):
        """
        http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTwebsite.html

        Configure bucket website with xml data.
        data may be a string or a dict.
        """
        return self.connection.bucket_request(
                'PUT',
                bucket,
                params="website",
                data=self._to_xml(data))

    @check_response
    def copy(self, remote_src, remote_dst, headers={}):
        """
        Copies remote_src to remote_dst.  headers may be used to
        change the properties of remote_dst including its metadata.

        If any metadata headers are included in headers then they
        become the metadata of remote_dst; otherwise the metadata
        of remote_src is copied.

        A metadata header is any header that starts with
        `S3Facts.amz_metadata_prefix`

        Returns on success, raises StorageError on failure.
        """
        return self.connection.copy(
                remote_src,
                remote_dst,
                headers=headers)

    @check_response
    def delete(self, remote_name):
        """
        Deletes remote_name from storage.
        Returns on success, raises StorageError on failure.
       """
        return self.connection.delete(remote_name)

    def exists(self, remote_name):
        """
        Tests if remote_name is stored in remote storage.

        Returns (exists, metadata)

        exists  
            True if remote_name exists, False otherwise.

        metadata 
            a dict.  The file's metadata (when exists
            == True)

        raises StorageError on failure.
        """
        try:
            response = self.connection.exists(remote_name)
            self.response = response.response
            if response.ok:
                return not response.error, response.metadata
            else:
                raise StorageError(
                        'exists', 
                        response.error, 
                        response=response.response)

        # Earlier, when the file does not exist, requests returned a 404
        # with a non-empty reason.  Something must have changed!
        #
        # Now a ChunkedEncodingError is raised with
        # "IncompleteRead(0 bytes read)"
        #
        except requests.models.ChunkedEncodingError, e:
            if str(e) == 'IncompleteRead(0 bytes read)':
                return False, {}
            else:
                raise StorageError('exists ChunkedEncodingError', e)
        except Exception, e:
            raise StorageError('exists ', e)

    def read(self, remote_name, local_name):
        """
        Reads (downloads) `remote_name` from storage and saves it
        in the local file system as `local_name`

        Returns metadata (a dict) on success, raises StorageError
        on failure.
        """
        try:
            # open a temporary, unique, dotfile in the same directory 
            # as `local_name`
            timestamp = time.strftime('%Y%m%d%H%M%S', time.localtime())
            dirname = os.path.dirname(os.path.normpath(local_name))
            tmp_filename = os.path.join(dirname, '.s3-%s.tmp' % timestamp)
            with open(tmp_filename, 'w+b') as fo:
                response = self.connection.read(remote_name, fo)
            self.response = response.response
            if response.ok:
                os.rename(tmp_filename, local_name)
                return response.metadata
            else:
                try:
                    os.unlink(tmp_filename)
                except OSError:
                    pass
                raise StorageError(
                        'read', 
                        response.error, 
                        response=response.response)
        except Exception, e:
            try:
                os.unlink(tmp_filename)
            except OSError:
                pass
            raise StorageError('read ', e)

    @check_response
    def update_metadata(self, remote_name, headers):
        """
        Updates the properties of remote_name by including headers
        in the request.

        Headers which begin with `S3Facts.amz_metadata_prefix` are
        used to set metadata.

        Returns on success, raises StorageError on failure.
        """
        return self.connection.update_metadata(
                remote_name,
                headers=headers)

    @check_response
    def write(self, local_name, remote_name, headers={}):
        """
        Writes (uploads) `local_name` from the local file system
        to storage as `remote_name`. `headers` may contain
        additional headers for the request.

        Headers which begin with `S3Facts.amz_metadata_prefix` are
        used to set metadata.

        Returns on success, raises StorageError on failure.
        """
        with open(local_name, 'r') as fi:
            return self.connection.write(
                    fi,
                    remote_name,
                    headers=headers)
